* 
* ==> Audit <==
* |--------------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|
|   Command    |              Args              | Profile  |       User        | Version |     Start Time      |      End Time       |
|--------------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 31 Jan 24 19:01 -03 | 31 Jan 24 19:03 -03 |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 01 Feb 24 18:56 -03 | 01 Feb 24 18:58 -03 |
| service      | k8s-teste                      | minikube | JONATHAN\joohnsro | v1.32.0 | 01 Feb 24 19:13 -03 | 01 Feb 24 19:14 -03 |
| service      | k8s-teste                      | minikube | JONATHAN\joohnsro | v1.32.0 | 01 Feb 24 19:19 -03 | 01 Feb 24 19:20 -03 |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 01 Feb 24 19:22 -03 |                     |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 01 Feb 24 19:23 -03 | 01 Feb 24 19:23 -03 |
| service      | k8s-teste                      | minikube | JONATHAN\joohnsro | v1.32.0 | 01 Feb 24 19:24 -03 | 01 Feb 24 19:26 -03 |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 01 Feb 24 19:27 -03 | 01 Feb 24 19:28 -03 |
| service      | k8s-teste                      | minikube | JONATHAN\joohnsro | v1.32.0 | 01 Feb 24 19:28 -03 | 01 Feb 24 19:29 -03 |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 02 Feb 24 10:28 -03 |                     |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 02 Feb 24 10:29 -03 | 02 Feb 24 10:29 -03 |
| service      | my-nginx --url                 | minikube | JONATHAN\joohnsro | v1.32.0 | 02 Feb 24 16:49 -03 | 02 Feb 24 16:49 -03 |
| service      | my-nginx --url                 | minikube | JONATHAN\joohnsro | v1.32.0 | 02 Feb 24 16:54 -03 | 02 Feb 24 17:00 -03 |
| service      | my-nginx --url                 | minikube | JONATHAN\joohnsro | v1.32.0 | 02 Feb 24 17:00 -03 | 02 Feb 24 17:01 -03 |
| service      | nginx-fiap --url               | minikube | JONATHAN\joohnsro | v1.32.0 | 02 Feb 24 17:03 -03 | 02 Feb 24 17:04 -03 |
| update-check |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 07 Feb 24 17:04 -03 | 07 Feb 24 17:04 -03 |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 07 Feb 24 17:42 -03 |                     |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 07 Feb 24 17:43 -03 | 07 Feb 24 17:43 -03 |
| service      | app-desafio-service --url      | minikube | JONATHAN\joohnsro | v1.32.0 | 07 Feb 24 18:35 -03 |                     |
| service      | service/app-desafio-service    | minikube | JONATHAN\joohnsro | v1.32.0 | 07 Feb 24 18:35 -03 |                     |
|              | --url                          |          |                   |         |                     |                     |
| service      | app-desafio-service --url      | minikube | JONATHAN\joohnsro | v1.32.0 | 07 Feb 24 18:35 -03 |                     |
| service      | test-service --url             | minikube | JONATHAN\joohnsro | v1.32.0 | 07 Feb 24 18:36 -03 |                     |
| update-check |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 08 Feb 24 17:04 -03 | 08 Feb 24 17:04 -03 |
| update-check |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 10 Feb 24 08:54 -03 | 10 Feb 24 08:54 -03 |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 10 Feb 24 08:55 -03 | 10 Feb 24 08:55 -03 |
| service      | app-desafio-service            | minikube | JONATHAN\joohnsro | v1.32.0 | 10 Feb 24 09:06 -03 | 10 Feb 24 09:30 -03 |
| service      | app-desafio-service            | minikube | JONATHAN\joohnsro | v1.32.0 | 10 Feb 24 09:32 -03 | 10 Feb 24 09:33 -03 |
| service      | app-desafio-service            | minikube | JONATHAN\joohnsro | v1.32.0 | 10 Feb 24 09:33 -03 | 10 Feb 24 09:34 -03 |
| service      | app-desafio-service            | minikube | JONATHAN\joohnsro | v1.32.0 | 10 Feb 24 09:34 -03 |                     |
| stop         |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 10 Feb 24 09:48 -03 | 10 Feb 24 09:48 -03 |
| update-check |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 17 Feb 24 09:06 -03 | 17 Feb 24 09:06 -03 |
| start        |                                | minikube | JONATHAN\joohnsro | v1.32.0 | 17 Feb 24 09:37 -03 | 17 Feb 24 09:38 -03 |
| service      | kubernetes-2-desafio-service   | minikube | JONATHAN\joohnsro | v1.32.0 | 17 Feb 24 09:57 -03 |                     |
|              | --url                          |          |                   |         |                     |                     |
| service      | kubernetes-2-desafio-service   | minikube | JONATHAN\joohnsro | v1.32.0 | 17 Feb 24 09:58 -03 |                     |
|              | --url                          |          |                   |         |                     |                     |
| service      | kubernetes-2-desafio-service   | minikube | JONATHAN\joohnsro | v1.32.0 | 17 Feb 24 09:58 -03 |                     |
|              | --url                          |          |                   |         |                     |                     |
|--------------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/02/17 09:37:52
Running on machine: JONATHAN
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0217 09:37:52.693811   17108 out.go:296] Setting OutFile to fd 1084 ...
I0217 09:37:52.704122   17108 out.go:343] TERM=xterm,COLORTERM=, which probably does not support color
I0217 09:37:52.704122   17108 out.go:309] Setting ErrFile to fd 1084...
I0217 09:37:52.704122   17108 out.go:343] TERM=xterm,COLORTERM=, which probably does not support color
W0217 09:37:52.714526   17108 root.go:314] Error reading config file at C:\Users\baque\.minikube\config\config.json: open C:\Users\baque\.minikube\config\config.json: The system cannot find the file specified.
I0217 09:37:52.720265   17108 out.go:303] Setting JSON to false
I0217 09:37:52.724948   17108 start.go:128] hostinfo: {"hostname":"JONATHAN","uptime":243488,"bootTime":1707929984,"procs":285,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.3155 Build 22621.3155","kernelVersion":"10.0.22621.3155 Build 22621.3155","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"15928b22-7a93-4676-8303-a35787b11dc3"}
W0217 09:37:52.724948   17108 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0217 09:37:52.726871   17108 out.go:177] * minikube v1.32.0 on Microsoft Windows 11 Home Single Language 10.0.22621.3155 Build 22621.3155
I0217 09:37:52.727916   17108 notify.go:220] Checking for updates...
I0217 09:37:52.735258   17108 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0217 09:37:52.736803   17108 driver.go:378] Setting default libvirt URI to qemu:///system
I0217 09:37:52.908727   17108 docker.go:122] docker version: linux-24.0.5:Docker Desktop 4.22.1 (118664)
I0217 09:37:52.928528   17108 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0217 09:37:53.247114   17108 info.go:266] docker info: {ID:07a25201-e08b-4198-8406-d09631374044 Containers:42 ContainersRunning:34 ContainersPaused:0 ContainersStopped:8 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:150 OomKillDisable:true NGoroutines:150 SystemTime:2024-02-17 12:37:53.2005134 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.4.72-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8294408192 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0217 09:37:53.248693   17108 out.go:177] * Using the docker driver based on existing profile
I0217 09:37:53.249216   17108 start.go:298] selected driver: docker
I0217 09:37:53.249216   17108 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\baque:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0217 09:37:53.249216   17108 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0217 09:37:53.290493   17108 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0217 09:37:53.583321   17108 info.go:266] docker info: {ID:07a25201-e08b-4198-8406-d09631374044 Containers:42 ContainersRunning:34 ContainersPaused:0 ContainersStopped:8 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:150 OomKillDisable:true NGoroutines:150 SystemTime:2024-02-17 12:37:53.539259 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.4.72-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8294408192 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0217 09:37:53.655667   17108 cni.go:84] Creating CNI manager for ""
I0217 09:37:53.655667   17108 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0217 09:37:53.656188   17108 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\baque:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0217 09:37:53.657743   17108 out.go:177] * Starting control plane node minikube in cluster minikube
I0217 09:37:53.658793   17108 cache.go:121] Beginning downloading kic base image for docker with docker
I0217 09:37:53.659317   17108 out.go:177] * Pulling base image ...
I0217 09:37:53.660363   17108 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0217 09:37:53.660363   17108 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0217 09:37:53.661976   17108 preload.go:148] Found local preload: C:\Users\baque\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0217 09:37:53.662507   17108 cache.go:56] Caching tarball of preloaded images
I0217 09:37:53.663035   17108 preload.go:174] Found C:\Users\baque\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0217 09:37:53.663035   17108 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0217 09:37:53.663035   17108 profile.go:148] Saving config to C:\Users\baque\.minikube\profiles\minikube\config.json ...
I0217 09:37:53.814181   17108 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0217 09:37:53.814181   17108 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0217 09:37:53.814685   17108 cache.go:194] Successfully downloaded all kic artifacts
I0217 09:37:53.814685   17108 start.go:365] acquiring machines lock for minikube: {Name:mk14c547736a30313c86f98a0af96508c9b7f6da Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0217 09:37:53.815200   17108 start.go:369] acquired machines lock for "minikube" in 515.4Âµs
I0217 09:37:53.815200   17108 start.go:96] Skipping create...Using existing machine configuration
I0217 09:37:53.815200   17108 fix.go:54] fixHost starting: 
I0217 09:37:53.856213   17108 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 09:37:53.982836   17108 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0217 09:37:53.982836   17108 fix.go:128] unexpected machine state, will restart: <nil>
I0217 09:37:53.983340   17108 out.go:177] * Restarting existing docker container for "minikube" ...
I0217 09:37:54.004736   17108 cli_runner.go:164] Run: docker start minikube
I0217 09:37:54.367763   17108 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 09:37:54.554959   17108 kic.go:430] container "minikube" state is running.
I0217 09:37:54.581035   17108 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0217 09:37:54.709344   17108 profile.go:148] Saving config to C:\Users\baque\.minikube\profiles\minikube\config.json ...
I0217 09:37:54.711426   17108 machine.go:88] provisioning docker machine ...
I0217 09:37:54.712510   17108 ubuntu.go:169] provisioning hostname "minikube"
I0217 09:37:54.738452   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:54.911276   17108 main.go:141] libmachine: Using SSH client type: native
I0217 09:37:54.923831   17108 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7e47e0] 0x7e7320 <nil>  [] 0s} 127.0.0.1 63536 <nil> <nil>}
I0217 09:37:54.923831   17108 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0217 09:37:55.054896   17108 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0217 09:37:55.083605   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:55.234041   17108 main.go:141] libmachine: Using SSH client type: native
I0217 09:37:55.234578   17108 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7e47e0] 0x7e7320 <nil>  [] 0s} 127.0.0.1 63536 <nil> <nil>}
I0217 09:37:55.234578   17108 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0217 09:37:55.316347   17108 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0217 09:37:55.316347   17108 ubuntu.go:175] set auth options {CertDir:C:\Users\baque\.minikube CaCertPath:C:\Users\baque\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\baque\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\baque\.minikube\machines\server.pem ServerKeyPath:C:\Users\baque\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\baque\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\baque\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\baque\.minikube}
I0217 09:37:55.316347   17108 ubuntu.go:177] setting up certificates
I0217 09:37:55.316347   17108 provision.go:83] configureAuth start
I0217 09:37:55.337278   17108 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0217 09:37:55.460928   17108 provision.go:138] copyHostCerts
I0217 09:37:55.466426   17108 exec_runner.go:144] found C:\Users\baque\.minikube/ca.pem, removing ...
I0217 09:37:55.466968   17108 exec_runner.go:203] rm: C:\Users\baque\.minikube\ca.pem
I0217 09:37:55.466968   17108 exec_runner.go:151] cp: C:\Users\baque\.minikube\certs\ca.pem --> C:\Users\baque\.minikube/ca.pem (1082 bytes)
I0217 09:37:55.473662   17108 exec_runner.go:144] found C:\Users\baque\.minikube/cert.pem, removing ...
I0217 09:37:55.473662   17108 exec_runner.go:203] rm: C:\Users\baque\.minikube\cert.pem
I0217 09:37:55.474173   17108 exec_runner.go:151] cp: C:\Users\baque\.minikube\certs\cert.pem --> C:\Users\baque\.minikube/cert.pem (1127 bytes)
I0217 09:37:55.479489   17108 exec_runner.go:144] found C:\Users\baque\.minikube/key.pem, removing ...
I0217 09:37:55.479489   17108 exec_runner.go:203] rm: C:\Users\baque\.minikube\key.pem
I0217 09:37:55.479489   17108 exec_runner.go:151] cp: C:\Users\baque\.minikube\certs\key.pem --> C:\Users\baque\.minikube/key.pem (1675 bytes)
I0217 09:37:55.479993   17108 provision.go:112] generating server cert: C:\Users\baque\.minikube\machines\server.pem ca-key=C:\Users\baque\.minikube\certs\ca.pem private-key=C:\Users\baque\.minikube\certs\ca-key.pem org=joohnsro.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0217 09:37:55.716931   17108 provision.go:172] copyRemoteCerts
I0217 09:37:55.750726   17108 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0217 09:37:55.773889   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:55.894912   17108 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63536 SSHKeyPath:C:\Users\baque\.minikube\machines\minikube\id_rsa Username:docker}
I0217 09:37:55.978907   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0217 09:37:55.999787   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\machines\server.pem --> /etc/docker/server.pem (1204 bytes)
I0217 09:37:56.020726   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0217 09:37:56.039627   17108 provision.go:86] duration metric: configureAuth took 723.2798ms
I0217 09:37:56.039627   17108 ubuntu.go:193] setting minikube options for container-runtime
I0217 09:37:56.040148   17108 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0217 09:37:56.062461   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:56.172388   17108 main.go:141] libmachine: Using SSH client type: native
I0217 09:37:56.173054   17108 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7e47e0] 0x7e7320 <nil>  [] 0s} 127.0.0.1 63536 <nil> <nil>}
I0217 09:37:56.173054   17108 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0217 09:37:56.240038   17108 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0217 09:37:56.240038   17108 ubuntu.go:71] root file system type: overlay
I0217 09:37:56.240564   17108 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0217 09:37:56.258868   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:56.372604   17108 main.go:141] libmachine: Using SSH client type: native
I0217 09:37:56.373121   17108 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7e47e0] 0x7e7320 <nil>  [] 0s} 127.0.0.1 63536 <nil> <nil>}
I0217 09:37:56.373121   17108 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0217 09:37:56.504917   17108 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0217 09:37:56.526472   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:56.648984   17108 main.go:141] libmachine: Using SSH client type: native
I0217 09:37:56.649621   17108 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7e47e0] 0x7e7320 <nil>  [] 0s} 127.0.0.1 63536 <nil> <nil>}
I0217 09:37:56.649621   17108 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0217 09:37:56.769485   17108 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0217 09:37:56.769485   17108 machine.go:91] provisioned docker machine in 2.0580584s
I0217 09:37:56.769485   17108 start.go:300] post-start starting for "minikube" (driver="docker")
I0217 09:37:56.769485   17108 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0217 09:37:56.795966   17108 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0217 09:37:56.814194   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:56.924847   17108 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63536 SSHKeyPath:C:\Users\baque\.minikube\machines\minikube\id_rsa Username:docker}
I0217 09:37:57.011826   17108 ssh_runner.go:195] Run: cat /etc/os-release
I0217 09:37:57.014962   17108 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0217 09:37:57.014962   17108 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0217 09:37:57.014962   17108 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0217 09:37:57.014962   17108 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0217 09:37:57.014962   17108 filesync.go:126] Scanning C:\Users\baque\.minikube\addons for local assets ...
I0217 09:37:57.014962   17108 filesync.go:126] Scanning C:\Users\baque\.minikube\files for local assets ...
I0217 09:37:57.014962   17108 start.go:303] post-start completed in 245.4773ms
I0217 09:37:57.016521   17108 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0217 09:37:57.035809   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:57.171701   17108 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63536 SSHKeyPath:C:\Users\baque\.minikube\machines\minikube\id_rsa Username:docker}
I0217 09:37:57.209972   17108 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0217 09:37:57.213604   17108 fix.go:56] fixHost completed within 3.3984045s
I0217 09:37:57.213604   17108 start.go:83] releasing machines lock for "minikube", held for 3.3984045s
I0217 09:37:57.231642   17108 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0217 09:37:57.342268   17108 ssh_runner.go:195] Run: cat /version.json
I0217 09:37:57.352073   17108 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0217 09:37:57.361378   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:57.373741   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:37:57.478775   17108 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63536 SSHKeyPath:C:\Users\baque\.minikube\machines\minikube\id_rsa Username:docker}
I0217 09:37:57.509356   17108 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63536 SSHKeyPath:C:\Users\baque\.minikube\machines\minikube\id_rsa Username:docker}
I0217 09:37:57.602140   17108 ssh_runner.go:195] Run: systemctl --version
I0217 09:37:58.218272   17108 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0217 09:37:58.251157   17108 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0217 09:37:58.260710   17108 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0217 09:37:58.286535   17108 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0217 09:37:58.294960   17108 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0217 09:37:58.294960   17108 start.go:472] detecting cgroup driver to use...
I0217 09:37:58.294960   17108 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0217 09:37:58.296001   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0217 09:37:58.309801   17108 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0217 09:37:58.319440   17108 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0217 09:37:58.326785   17108 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0217 09:37:58.328390   17108 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0217 09:37:58.337754   17108 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0217 09:37:58.346702   17108 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0217 09:37:58.355964   17108 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0217 09:37:58.364895   17108 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0217 09:37:58.374593   17108 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0217 09:37:58.409084   17108 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0217 09:37:58.444142   17108 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0217 09:37:58.477542   17108 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 09:37:58.552291   17108 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0217 09:37:58.603261   17108 start.go:472] detecting cgroup driver to use...
I0217 09:37:58.603261   17108 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0217 09:37:58.633069   17108 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0217 09:37:58.644782   17108 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0217 09:37:58.675220   17108 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0217 09:37:58.688992   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0217 09:37:58.705646   17108 ssh_runner.go:195] Run: which cri-dockerd
I0217 09:37:58.740529   17108 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0217 09:37:58.754220   17108 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0217 09:37:58.797092   17108 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0217 09:37:58.874797   17108 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0217 09:37:58.921914   17108 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0217 09:37:58.921914   17108 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0217 09:37:58.967304   17108 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 09:37:59.040375   17108 ssh_runner.go:195] Run: sudo systemctl restart docker
I0217 09:37:59.260603   17108 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0217 09:37:59.331854   17108 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0217 09:37:59.404620   17108 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0217 09:37:59.476135   17108 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 09:37:59.548120   17108 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0217 09:37:59.585253   17108 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 09:37:59.656273   17108 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0217 09:37:59.818295   17108 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0217 09:37:59.820918   17108 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0217 09:37:59.824676   17108 start.go:540] Will wait 60s for crictl version
I0217 09:37:59.826228   17108 ssh_runner.go:195] Run: which crictl
I0217 09:37:59.855318   17108 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0217 09:37:59.948629   17108 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0217 09:37:59.966440   17108 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0217 09:38:00.048515   17108 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0217 09:38:00.067082   17108 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0217 09:38:00.086257   17108 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0217 09:38:00.259622   17108 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0217 09:38:00.261736   17108 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0217 09:38:00.265322   17108 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0217 09:38:00.295215   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0217 09:38:00.415898   17108 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0217 09:38:00.435293   17108 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0217 09:38:00.450854   17108 docker.go:671] Got preloaded images: -- stdout --
joohnsro/app-desafio:1.1
joohnsro/app-desafio:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
joohnsro/app-node:1.2
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0217 09:38:00.450854   17108 docker.go:601] Images already preloaded, skipping extraction
I0217 09:38:00.469865   17108 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0217 09:38:00.484201   17108 docker.go:671] Got preloaded images: -- stdout --
joohnsro/app-desafio:1.1
joohnsro/app-desafio:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
joohnsro/app-node:1.2
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0217 09:38:00.484201   17108 cache_images.go:84] Images are preloaded, skipping loading
I0217 09:38:00.502932   17108 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0217 09:38:00.636271   17108 cni.go:84] Creating CNI manager for ""
I0217 09:38:00.638309   17108 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0217 09:38:00.638829   17108 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0217 09:38:00.638829   17108 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0217 09:38:00.638829   17108 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0217 09:38:00.638829   17108 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0217 09:38:00.664771   17108 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0217 09:38:00.674379   17108 binaries.go:44] Found k8s binaries, skipping transfer
I0217 09:38:00.700676   17108 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0217 09:38:00.708102   17108 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0217 09:38:00.724534   17108 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0217 09:38:00.740790   17108 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0217 09:38:00.756538   17108 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0217 09:38:00.760195   17108 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0217 09:38:00.769168   17108 certs.go:56] Setting up C:\Users\baque\.minikube\profiles\minikube for IP: 192.168.49.2
I0217 09:38:00.769689   17108 certs.go:190] acquiring lock for shared ca certs: {Name:mk48a67f420198f5bea5ce9f0471890aa3a17f04 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0217 09:38:00.779616   17108 certs.go:199] skipping minikubeCA CA generation: C:\Users\baque\.minikube\ca.key
I0217 09:38:00.789034   17108 certs.go:199] skipping proxyClientCA CA generation: C:\Users\baque\.minikube\proxy-client-ca.key
I0217 09:38:00.800673   17108 certs.go:315] skipping minikube-user signed cert generation: C:\Users\baque\.minikube\profiles\minikube\client.key
I0217 09:38:00.812770   17108 certs.go:315] skipping minikube signed cert generation: C:\Users\baque\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0217 09:38:00.822715   17108 certs.go:315] skipping aggregator signed cert generation: C:\Users\baque\.minikube\profiles\minikube\proxy-client.key
I0217 09:38:00.825386   17108 certs.go:437] found cert: C:\Users\baque\.minikube\certs\C:\Users\baque\.minikube\certs\ca-key.pem (1675 bytes)
I0217 09:38:00.825386   17108 certs.go:437] found cert: C:\Users\baque\.minikube\certs\C:\Users\baque\.minikube\certs\ca.pem (1082 bytes)
I0217 09:38:00.825386   17108 certs.go:437] found cert: C:\Users\baque\.minikube\certs\C:\Users\baque\.minikube\certs\cert.pem (1127 bytes)
I0217 09:38:00.825386   17108 certs.go:437] found cert: C:\Users\baque\.minikube\certs\C:\Users\baque\.minikube\certs\key.pem (1675 bytes)
I0217 09:38:00.829591   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0217 09:38:00.850312   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0217 09:38:00.868801   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0217 09:38:00.888976   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0217 09:38:00.907027   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0217 09:38:00.927197   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0217 09:38:00.946539   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0217 09:38:00.966654   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0217 09:38:00.985531   17108 ssh_runner.go:362] scp C:\Users\baque\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0217 09:38:01.008515   17108 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0217 09:38:01.023306   17108 ssh_runner.go:195] Run: openssl version
I0217 09:38:01.059758   17108 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0217 09:38:01.070198   17108 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0217 09:38:01.073896   17108 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jan 31 22:03 /usr/share/ca-certificates/minikubeCA.pem
I0217 09:38:01.074928   17108 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0217 09:38:01.109380   17108 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0217 09:38:01.119417   17108 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0217 09:38:01.123152   17108 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0217 09:38:01.129552   17108 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0217 09:38:01.136282   17108 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0217 09:38:01.143134   17108 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0217 09:38:01.149380   17108 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0217 09:38:01.155982   17108 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0217 09:38:01.161696   17108 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\baque:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0217 09:38:01.180395   17108 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0217 09:38:01.220801   17108 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0217 09:38:01.228274   17108 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0217 09:38:01.228857   17108 kubeadm.go:636] restartCluster start
I0217 09:38:01.255811   17108 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0217 09:38:01.262868   17108 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0217 09:38:01.281076   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0217 09:38:01.399091   17108 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\baque\.kube\config
I0217 09:38:01.399609   17108 kubeconfig.go:146] "minikube" context is missing from C:\Users\baque\.kube\config - will repair!
I0217 09:38:01.400126   17108 lock.go:35] WriteFile acquiring C:\Users\baque\.kube\config: {Name:mk12eb48581dafb5d6410fcf7044f6ec4e64ebd6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0217 09:38:01.444675   17108 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0217 09:38:01.453876   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:01.480761   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:01.491301   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:01.491301   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:01.518113   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:01.526397   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:02.036518   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:02.063389   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:02.072047   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:02.531670   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:02.560410   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:02.569892   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:03.029361   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:03.058327   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:03.067108   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:03.539827   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:03.566058   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:03.574824   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:04.032668   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:04.089846   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:04.103273   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:04.540984   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:04.569735   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:04.577675   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:05.036707   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:05.065090   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:05.078051   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:05.534104   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:05.562486   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:05.571610   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:06.031105   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:06.059128   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:06.070053   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:06.529011   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:06.555925   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:06.564911   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:07.040615   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:07.066097   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:07.074842   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:07.539086   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:07.572495   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:07.581999   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:08.036598   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:08.066477   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:08.077505   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:08.532724   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:08.558766   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:08.567005   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:09.031446   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:09.061355   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:09.070442   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:09.530345   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:09.555967   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:09.564697   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:10.030704   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:10.056211   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:10.064670   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:10.540084   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:10.564754   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:10.574299   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:11.034071   17108 api_server.go:166] Checking apiserver status ...
I0217 09:38:11.059531   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 09:38:11.067704   17108 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0217 09:38:11.463127   17108 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0217 09:38:11.463127   17108 kubeadm.go:1128] stopping kube-system containers ...
I0217 09:38:11.481035   17108 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0217 09:38:11.498153   17108 docker.go:469] Stopping containers: [e12210b164e4 02827088f125 3a421eda8f59 fb0bfb185814 275f4ce5f891 697e1a0dab96 14433f552f49 9a94d984dcb4 37482745d9af af70fe2effca 4aa9ac44a818 d09d0c1f8446 6bb657cdf472 bb7aecffbfeb f06549150a7f 33e072e9f673 c8abcc609feb bd84b1631baf f39683c148a1 0803b27958c3 bca6e18efcdc e867b332c96c ab0d259392ad 3663e8f7047c a5062dd5feca ce0e7a42795d f061d5e26b96]
I0217 09:38:11.516923   17108 ssh_runner.go:195] Run: docker stop e12210b164e4 02827088f125 3a421eda8f59 fb0bfb185814 275f4ce5f891 697e1a0dab96 14433f552f49 9a94d984dcb4 37482745d9af af70fe2effca 4aa9ac44a818 d09d0c1f8446 6bb657cdf472 bb7aecffbfeb f06549150a7f 33e072e9f673 c8abcc609feb bd84b1631baf f39683c148a1 0803b27958c3 bca6e18efcdc e867b332c96c ab0d259392ad 3663e8f7047c a5062dd5feca ce0e7a42795d f061d5e26b96
I0217 09:38:11.564361   17108 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0217 09:38:11.603281   17108 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0217 09:38:11.611894   17108 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Feb  1 21:58 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Feb 10 11:55 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5655 Feb  1 21:58 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Feb 10 11:55 /etc/kubernetes/scheduler.conf

I0217 09:38:11.642895   17108 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0217 09:38:11.682607   17108 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0217 09:38:11.717461   17108 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0217 09:38:11.726865   17108 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0217 09:38:11.754208   17108 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0217 09:38:11.788040   17108 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0217 09:38:11.796525   17108 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0217 09:38:11.823118   17108 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0217 09:38:11.858444   17108 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0217 09:38:11.867222   17108 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0217 09:38:11.867222   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0217 09:38:11.985955   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0217 09:38:12.477502   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0217 09:38:12.568297   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0217 09:38:12.604464   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0217 09:38:12.641748   17108 api_server.go:52] waiting for apiserver process to appear ...
I0217 09:38:12.668744   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 09:38:12.704745   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 09:38:13.242796   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 09:38:13.741955   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 09:38:14.261256   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 09:38:14.277723   17108 api_server.go:72] duration metric: took 1.6359749s to wait for apiserver process to appear ...
I0217 09:38:14.277723   17108 api_server.go:88] waiting for apiserver healthz status ...
I0217 09:38:14.277723   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:14.280392   17108 api_server.go:269] stopped: https://127.0.0.1:63535/healthz: Get "https://127.0.0.1:63535/healthz": EOF
I0217 09:38:14.280392   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:14.284093   17108 api_server.go:269] stopped: https://127.0.0.1:63535/healthz: Get "https://127.0.0.1:63535/healthz": EOF
I0217 09:38:14.785745   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:16.561216   17108 api_server.go:279] https://127.0.0.1:63535/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0217 09:38:16.561758   17108 api_server.go:103] status: https://127.0.0.1:63535/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0217 09:38:16.561758   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:16.660092   17108 api_server.go:279] https://127.0.0.1:63535/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0217 09:38:16.660602   17108 api_server.go:103] status: https://127.0.0.1:63535/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0217 09:38:16.788953   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:16.795816   17108 api_server.go:279] https://127.0.0.1:63535/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0217 09:38:16.795816   17108 api_server.go:103] status: https://127.0.0.1:63535/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0217 09:38:17.286029   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:17.359630   17108 api_server.go:279] https://127.0.0.1:63535/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0217 09:38:17.359630   17108 api_server.go:103] status: https://127.0.0.1:63535/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0217 09:38:17.786319   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:17.856835   17108 api_server.go:279] https://127.0.0.1:63535/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0217 09:38:17.856835   17108 api_server.go:103] status: https://127.0.0.1:63535/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0217 09:38:18.298663   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:18.305186   17108 api_server.go:279] https://127.0.0.1:63535/healthz returned 200:
ok
I0217 09:38:18.320108   17108 api_server.go:141] control plane version: v1.28.3
I0217 09:38:18.320108   17108 api_server.go:131] duration metric: took 4.0423851s to wait for apiserver health ...
I0217 09:38:18.320108   17108 cni.go:84] Creating CNI manager for ""
I0217 09:38:18.320108   17108 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0217 09:38:18.320627   17108 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0217 09:38:18.350477   17108 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0217 09:38:18.361270   17108 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0217 09:38:18.385621   17108 system_pods.go:43] waiting for kube-system pods to appear ...
I0217 09:38:18.403423   17108 system_pods.go:59] 7 kube-system pods found
I0217 09:38:18.403423   17108 system_pods.go:61] "coredns-5dd5756b68-dn7kp" [ce187499-67f4-42cf-999f-ce33b52a252c] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0217 09:38:18.403423   17108 system_pods.go:61] "etcd-minikube" [9c1dbcf3-76e3-4665-b9e6-daf39e79bc57] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0217 09:38:18.403423   17108 system_pods.go:61] "kube-apiserver-minikube" [eb5bb444-74f8-4926-ac3c-6e3c40e97a7f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0217 09:38:18.403423   17108 system_pods.go:61] "kube-controller-manager-minikube" [5c17c44b-20e0-4e41-85ef-26f6a7a4775f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0217 09:38:18.403423   17108 system_pods.go:61] "kube-proxy-9ck26" [36cb6bc2-05d6-47ea-8285-e41c62773b54] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0217 09:38:18.403423   17108 system_pods.go:61] "kube-scheduler-minikube" [9f5fa206-6f51-41d2-8dae-dc8ce1ba2dbb] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0217 09:38:18.403423   17108 system_pods.go:61] "storage-provisioner" [eb385375-03c1-47d8-aab1-002d7ee3f4c1] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0217 09:38:18.403423   17108 system_pods.go:74] duration metric: took 17.8022ms to wait for pod list to return data ...
I0217 09:38:18.403423   17108 node_conditions.go:102] verifying NodePressure condition ...
I0217 09:38:18.454590   17108 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0217 09:38:18.454590   17108 node_conditions.go:123] node cpu capacity is 12
I0217 09:38:18.455135   17108 node_conditions.go:105] duration metric: took 51.7125ms to run NodePressure ...
I0217 09:38:18.455135   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0217 09:38:18.760424   17108 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0217 09:38:18.768455   17108 ops.go:34] apiserver oom_adj: -16
I0217 09:38:18.768981   17108 kubeadm.go:640] restartCluster took 17.5395988s
I0217 09:38:18.768981   17108 kubeadm.go:406] StartCluster complete in 17.6072855s
I0217 09:38:18.768981   17108 settings.go:142] acquiring lock: {Name:mkb8fe233370325c498e41f7c26894896b669ac3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0217 09:38:18.768981   17108 settings.go:150] Updating kubeconfig:  C:\Users\baque\.kube\config
I0217 09:38:18.770591   17108 lock.go:35] WriteFile acquiring C:\Users\baque\.kube\config: {Name:mk12eb48581dafb5d6410fcf7044f6ec4e64ebd6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0217 09:38:18.771643   17108 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0217 09:38:18.772157   17108 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0217 09:38:18.772157   17108 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0217 09:38:18.772675   17108 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0217 09:38:18.772675   17108 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0217 09:38:18.772675   17108 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0217 09:38:18.772675   17108 addons.go:240] addon storage-provisioner should already be in state true
I0217 09:38:18.772675   17108 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0217 09:38:18.773223   17108 host.go:66] Checking if "minikube" exists ...
I0217 09:38:18.794154   17108 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0217 09:38:18.794679   17108 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0217 09:38:18.795779   17108 out.go:177] * Verifying Kubernetes components...
I0217 09:38:18.831619   17108 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 09:38:18.831619   17108 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 09:38:18.837418   17108 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0217 09:38:18.997777   17108 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0217 09:38:18.997777   17108 addons.go:240] addon default-storageclass should already be in state true
I0217 09:38:18.998304   17108 host.go:66] Checking if "minikube" exists ...
I0217 09:38:19.011463   17108 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0217 09:38:19.012027   17108 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0217 09:38:19.012027   17108 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0217 09:38:19.035929   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:38:19.046061   17108 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 09:38:19.059433   17108 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0217 09:38:19.087343   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0217 09:38:19.179712   17108 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63536 SSHKeyPath:C:\Users\baque\.minikube\machines\minikube\id_rsa Username:docker}
I0217 09:38:19.192270   17108 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0217 09:38:19.192270   17108 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0217 09:38:19.234633   17108 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 09:38:19.271596   17108 api_server.go:52] waiting for apiserver process to appear ...
I0217 09:38:19.326463   17108 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 09:38:19.334474   17108 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0217 09:38:19.339756   17108 api_server.go:72] duration metric: took 545.0769ms to wait for apiserver process to appear ...
I0217 09:38:19.339756   17108 api_server.go:88] waiting for apiserver healthz status ...
I0217 09:38:19.339756   17108 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63535/healthz ...
I0217 09:38:19.346609   17108 api_server.go:279] https://127.0.0.1:63535/healthz returned 200:
ok
I0217 09:38:19.348236   17108 api_server.go:141] control plane version: v1.28.3
I0217 09:38:19.348236   17108 api_server.go:131] duration metric: took 8.48ms to wait for apiserver health ...
I0217 09:38:19.348236   17108 system_pods.go:43] waiting for kube-system pods to appear ...
I0217 09:38:19.355590   17108 system_pods.go:59] 7 kube-system pods found
I0217 09:38:19.355590   17108 system_pods.go:61] "coredns-5dd5756b68-dn7kp" [ce187499-67f4-42cf-999f-ce33b52a252c] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0217 09:38:19.355590   17108 system_pods.go:61] "etcd-minikube" [9c1dbcf3-76e3-4665-b9e6-daf39e79bc57] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0217 09:38:19.355590   17108 system_pods.go:61] "kube-apiserver-minikube" [eb5bb444-74f8-4926-ac3c-6e3c40e97a7f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0217 09:38:19.355590   17108 system_pods.go:61] "kube-controller-manager-minikube" [5c17c44b-20e0-4e41-85ef-26f6a7a4775f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0217 09:38:19.355590   17108 system_pods.go:61] "kube-proxy-9ck26" [36cb6bc2-05d6-47ea-8285-e41c62773b54] Running
I0217 09:38:19.355590   17108 system_pods.go:61] "kube-scheduler-minikube" [9f5fa206-6f51-41d2-8dae-dc8ce1ba2dbb] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0217 09:38:19.355590   17108 system_pods.go:61] "storage-provisioner" [eb385375-03c1-47d8-aab1-002d7ee3f4c1] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0217 09:38:19.355590   17108 system_pods.go:74] duration metric: took 7.3543ms to wait for pod list to return data ...
I0217 09:38:19.355590   17108 kubeadm.go:581] duration metric: took 560.9112ms to wait for : map[apiserver:true system_pods:true] ...
I0217 09:38:19.355590   17108 node_conditions.go:102] verifying NodePressure condition ...
I0217 09:38:19.359287   17108 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0217 09:38:19.359287   17108 node_conditions.go:123] node cpu capacity is 12
I0217 09:38:19.359287   17108 node_conditions.go:105] duration metric: took 3.6968ms to run NodePressure ...
I0217 09:38:19.359287   17108 start.go:228] waiting for startup goroutines ...
I0217 09:38:19.410143   17108 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63536 SSHKeyPath:C:\Users\baque\.minikube\machines\minikube\id_rsa Username:docker}
I0217 09:38:19.508134   17108 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0217 09:38:19.908479   17108 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0217 09:38:19.908479   17108 addons.go:502] enable addons completed in 1.1368358s: enabled=[storage-provisioner default-storageclass]
I0217 09:38:19.908479   17108 start.go:233] waiting for cluster config update ...
I0217 09:38:19.908479   17108 start.go:242] writing updated cluster config ...
I0217 09:38:19.911631   17108 ssh_runner.go:195] Run: rm -f paused
I0217 09:38:20.017111   17108 start.go:600] kubectl: 1.27.2, cluster: 1.28.3 (minor skew: 1)
I0217 09:38:20.017654   17108 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.081770700Z" level=info msg="Starting up"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.090308700Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.101319200Z" level=info msg="Loading containers: start."
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.193342900Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.216705700Z" level=info msg="Loading containers: done."
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.225830600Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.225864800Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.225871600Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.225874900Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.225887800Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.225923200Z" level=info msg="Daemon has completed initialization"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.233364200Z" level=info msg="API listen on /var/run/docker.sock"
Feb 17 12:37:59 minikube dockerd[948]: time="2024-02-17T12:37:59.233403800Z" level=info msg="API listen on [::]:2376"
Feb 17 12:37:59 minikube systemd[1]: Started Docker Application Container Engine.
Feb 17 12:37:59 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Start docker client with request timeout 0s"
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Loaded network plugin cni"
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Docker Info: &{ID:68e57556-2614-4a70-89d0-58955b09e342 Containers:29 ContainersRunning:0 ContainersPaused:0 ContainersStopped:29 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:35 SystemTime:2024-02-17T12:37:59.8089832Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.4.72-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000569e30 NCPU:12 MemTotal:8294408192 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Setting cgroupDriver cgroupfs"
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 17 12:37:59 minikube cri-dockerd[1186]: time="2024-02-17T12:37:59Z" level=info msg="Start cri-dockerd grpc backend"
Feb 17 12:37:59 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 17 12:38:12 minikube cri-dockerd[1186]: time="2024-02-17T12:38:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-dn7kp_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"275f4ce5f891bff0a346859e657837fe0cba667293bffaa1853f4e99bffd2294\""
Feb 17 12:38:12 minikube cri-dockerd[1186]: time="2024-02-17T12:38:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-dn7kp_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bd84b1631bafcd162c1bb2eeb17266c7a5ff423e32478e93b46184f32485251f\""
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"app-desafio-deployment-647cf88bfd-5btcr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6c3edbf8fa6c17034f9cde21fefe6fd67e664f54637cb2295a9deaa654bc68e4\""
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"ce0e7a42795dab9fd1f1e8f0d25d50da4cb8f902f49a6b81000c0df4162feb72\". Proceed without further sandbox information."
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"3663e8f7047cb47dd1857424d11e099bbbc90eac9fada0f19326270ff117dcee\". Proceed without further sandbox information."
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a5062dd5feca677b8473665bda0545ccae7fe30b7def70e5abe143abea8d5605\". Proceed without further sandbox information."
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"f061d5e26b96bdb6b5467182b70edbcc7b199dd367d94981b4149cd0bc56b677\". Proceed without further sandbox information."
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9abae7e3299ebac6c1517476eadea76a03b8db81cea94898edb05caf7a7a2ade/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f667db02a70a0ca4cfb7fb628bcbf7dd3d56d5bd558e99a8aefb80f61ad4be6a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5d0ea84613461bf1b58c5bfe7b07ab4a88106edb134207db08eab3e794fdb5dd/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 17 12:38:13 minikube cri-dockerd[1186]: time="2024-02-17T12:38:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68bbd7700d8fffdd1aae180fb669e8c65705a841a164afd669ac7a455dc986a4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 17 12:38:14 minikube cri-dockerd[1186]: time="2024-02-17T12:38:14Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-dn7kp_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"275f4ce5f891bff0a346859e657837fe0cba667293bffaa1853f4e99bffd2294\""
Feb 17 12:38:16 minikube cri-dockerd[1186]: time="2024-02-17T12:38:16Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 17 12:38:17 minikube cri-dockerd[1186]: time="2024-02-17T12:38:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5cb1735c3e54f9095e808b49e9203f7300e719adf6d3c4d48e2b5174cb42b26f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 17 12:38:17 minikube cri-dockerd[1186]: time="2024-02-17T12:38:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/08c8233fec21a40b8a9309ec9e20fd17a28584872079450a1ebdb97af193dfa9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 17 12:38:17 minikube cri-dockerd[1186]: time="2024-02-17T12:38:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/86a6848926c2035063ed537f35e8a33e584386d4d35b0f89a94cf8cacce1717b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 17 12:38:17 minikube cri-dockerd[1186]: time="2024-02-17T12:38:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/09bad381021784352e0222f7413e7ec25d8cedaaef2b88b92d70ecda5bcf4fdd/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 17 12:38:18 minikube dockerd[948]: time="2024-02-17T12:38:18.156743400Z" level=info msg="ignoring event" container=db8dc47c487694a52ace119587d91b53c6eb26300c0f28d5d8801109375aca0f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 17 12:38:51 minikube cri-dockerd[1186]: time="2024-02-17T12:38:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a7f83c72987e03f7b26a7f08ec7d1f7fc785808580ca2ddf405a7ec464f48457/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 17 12:39:21 minikube dockerd[948]: time="2024-02-17T12:39:21.474346800Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=3f7df14d75f2b6f17eaca8efe7676be56420846b85877630b962c99e57ddcec7
Feb 17 12:39:21 minikube dockerd[948]: time="2024-02-17T12:39:21.499700900Z" level=info msg="ignoring event" container=3f7df14d75f2b6f17eaca8efe7676be56420846b85877630b962c99e57ddcec7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 17 12:39:21 minikube dockerd[948]: time="2024-02-17T12:39:21.615252900Z" level=info msg="ignoring event" container=86a6848926c2035063ed537f35e8a33e584386d4d35b0f89a94cf8cacce1717b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 17 12:40:26 minikube dockerd[948]: time="2024-02-17T12:40:26.931851200Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694
Feb 17 12:40:26 minikube dockerd[948]: time="2024-02-17T12:40:26.949664000Z" level=info msg="ignoring event" container=b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 17 12:40:27 minikube dockerd[948]: time="2024-02-17T12:40:27.064620600Z" level=info msg="ignoring event" container=a7f83c72987e03f7b26a7f08ec7d1f7fc785808580ca2ddf405a7ec464f48457 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 17 12:43:54 minikube cri-dockerd[1186]: time="2024-02-17T12:43:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e7ad0aeedd8e88bfa605d75ba6571e1b6ba0a4561f36b781dd761319fafa1ee0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 17 12:43:56 minikube dockerd[948]: time="2024-02-17T12:43:56.303871800Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Feb 17 12:44:11 minikube dockerd[948]: time="2024-02-17T12:44:11.730122900Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Feb 17 12:44:41 minikube dockerd[948]: time="2024-02-17T12:44:41.712968500Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Feb 17 12:45:32 minikube dockerd[948]: time="2024-02-17T12:45:32.917825100Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Feb 17 12:46:54 minikube cri-dockerd[1186]: time="2024-02-17T12:46:54Z" level=info msg="Stop pulling image joohnsro/kubernetes-2-desafio:1.0: Status: Downloaded newer image for joohnsro/kubernetes-2-desafio:1.0"
Feb 17 12:47:07 minikube dockerd[948]: time="2024-02-17T12:47:07.760764400Z" level=info msg="ignoring event" container=d34c80773b8105ca5dcf5a693aaf6f18e1eb748a3711bfff3660c328ce26ca2d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 17 12:47:07 minikube dockerd[948]: time="2024-02-17T12:47:07.885476100Z" level=info msg="ignoring event" container=e7ad0aeedd8e88bfa605d75ba6571e1b6ba0a4561f36b781dd761319fafa1ee0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 17 12:47:16 minikube cri-dockerd[1186]: time="2024-02-17T12:47:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2c14773c51e84eeaa121a6f442540414e2e1128e128720f4bb69a926172e1179/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                       ATTEMPT             POD ID              POD
5923a32234c70       add0ac19385a4       11 minutes ago      Running             kubernetes-2-desafio-pod   0                   2c14773c51e84       kubernetes-2-desafio-pod
7e2645562d63f       6e38f40d628db       20 minutes ago      Running             storage-provisioner        15                  08c8233fec21a       storage-provisioner
0ac69fc732c23       ead0a4a53df89       20 minutes ago      Running             coredns                    7                   09bad38102178       coredns-5dd5756b68-dn7kp
fbedc6f5ed443       bfc896cf80fba       20 minutes ago      Running             kube-proxy                 7                   5cb1735c3e54f       kube-proxy-9ck26
db8dc47c48769       6e38f40d628db       20 minutes ago      Exited              storage-provisioner        14                  08c8233fec21a       storage-provisioner
6dd0f943bc1ff       10baa1ca17068       20 minutes ago      Running             kube-controller-manager    7                   9abae7e3299eb       kube-controller-manager-minikube
373b0edb5828d       5374347291230       20 minutes ago      Running             kube-apiserver             7                   f667db02a70a0       kube-apiserver-minikube
20b3f7a21777f       6d1b4fd1b182d       20 minutes ago      Running             kube-scheduler             7                   5d0ea84613461       kube-scheduler-minikube
5bbd7aab1fcb0       73deb9a3f7025       20 minutes ago      Running             etcd                       7                   68bbd7700d8ff       etcd-minikube
02827088f125d       ead0a4a53df89       7 days ago          Exited              coredns                    6                   275f4ce5f891b       coredns-5dd5756b68-dn7kp
fb0bfb185814e       bfc896cf80fba       7 days ago          Exited              kube-proxy                 6                   697e1a0dab960       kube-proxy-9ck26
9a94d984dcb48       10baa1ca17068       7 days ago          Exited              kube-controller-manager    6                   bb7aecffbfeb3       kube-controller-manager-minikube
37482745d9afe       5374347291230       7 days ago          Exited              kube-apiserver             6                   6bb657cdf4720       kube-apiserver-minikube
af70fe2effca1       6d1b4fd1b182d       7 days ago          Exited              kube-scheduler             6                   d09d0c1f8446f       kube-scheduler-minikube
4aa9ac44a8180       73deb9a3f7025       7 days ago          Exited              etcd                       6                   f06549150a7fa       etcd-minikube

* 
* ==> coredns [02827088f125] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:44092 - 43532 "HINFO IN 1085442312588970071.7986569277230273007. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.1034739s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [0ac69fc732c2] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:58089 - 55477 "HINFO IN 8169646710113401141.2990415383616939754. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.0818075s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_31T19_03_22_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 31 Jan 2024 22:03:19 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 17 Feb 2024 12:58:52 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 17 Feb 2024 12:57:30 +0000   Wed, 31 Jan 2024 22:03:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 17 Feb 2024 12:57:30 +0000   Wed, 31 Jan 2024 22:03:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 17 Feb 2024 12:57:30 +0000   Wed, 31 Jan 2024 22:03:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 17 Feb 2024 12:57:30 +0000   Wed, 31 Jan 2024 22:03:32 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  263174212Ki
  hugepages-2Mi:      0
  memory:             8100008Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  263174212Ki
  hugepages-2Mi:      0
  memory:             8100008Ki
  pods:               110
System Info:
  Machine ID:                 23c8000d69e848d5b4e8251d37051d14
  System UUID:                23c8000d69e848d5b4e8251d37051d14
  Boot ID:                    393a9e2a-7219-46eb-9e37-da3772be3ddd
  Kernel Version:             5.4.72-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     kubernetes-2-desafio-pod            500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     11m
  kube-system                 coredns-5dd5756b68-dn7kp            100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     16d
  kube-system                 etcd-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         16d
  kube-system                 kube-apiserver-minikube             250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  kube-system                 kube-controller-manager-minikube    200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  kube-system                 kube-proxy-9ck26                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  kube-system                 kube-scheduler-minikube             100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1250m (10%!)(MISSING)  500m (4%!)(MISSING)
  memory             298Mi (3%!)(MISSING)   298Mi (3%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                  From             Message
  ----    ------                   ----                 ----             -------
  Normal  Starting                 7d1h                 kube-proxy       
  Normal  Starting                 20m                  kube-proxy       
  Normal  Starting                 7d1h                 kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  7d1h                 kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasNoDiskPressure    7d1h (x8 over 7d1h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     7d1h (x7 over 7d1h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  7d1h (x8 over 7d1h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  RegisteredNode           7d1h                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 20m                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientPID     20m (x7 over 20m)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  20m                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  20m (x8 over 20m)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    20m (x8 over 20m)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  RegisteredNode           20m                  node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Feb17 12:23] PCI: Fatal: No config space access function found
[  +0.010851] PCI: System does not support PCI
[  +0.066475] kvm: already loaded the other module
[  +0.025908] dxgk:err: dxg_drv_init  Version: 1
[  +0.000175] hv_utils: cannot register PTP clock: 0
[  +0.009833] Unstable clock detected, switching default tracing clock to "global"
              If you want to keep using the local clock, then add:
                "trace_clock=local"
              on the kernel command line
[  +3.025928] FS-Cache: Duplicate cookie detected
[  +0.000002] FS-Cache: O-cookie c=00000000f369cf19 [p=0000000003f4a5c0 fl=222 nc=0 na=1]
[  +0.000000] FS-Cache: O-cookie d=0000000068b47bb5 n=0000000011522b42
[  +0.000001] FS-Cache: O-key=[10] '34323934393337363132'
[  +0.000001] FS-Cache: N-cookie c=000000002af0b2a5 [p=0000000003f4a5c0 fl=2 nc=0 na=1]
[  +0.000001] FS-Cache: N-cookie d=0000000068b47bb5 n=000000001bca4645
[  +0.000000] FS-Cache: N-key=[10] '34323934393337363132'
[  +0.000259] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000001]  failed 2
[  +0.040274] WARNING: /usr/share/zoneinfo/America/Sao_Paulo not found. Is the tzdata package installed?
[  +1.529069] FS-Cache: Duplicate cookie detected
[  +0.000002] FS-Cache: O-cookie c=0000000064e0a3a3 [p=0000000003f4a5c0 fl=222 nc=0 na=1]
[  +0.000000] FS-Cache: O-cookie d=0000000068b47bb5 n=00000000e6c661e8
[  +0.000000] FS-Cache: O-key=[10] '34323934393337373639'
[  +0.000002] FS-Cache: N-cookie c=00000000db1b62a9 [p=0000000003f4a5c0 fl=2 nc=0 na=1]
[  +0.000000] FS-Cache: N-cookie d=0000000068b47bb5 n=00000000c5ab3189
[  +0.000001] FS-Cache: N-key=[10] '34323934393337373639'
[  +0.000176] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000001]  failed 2
[  +0.000699] init: (2) ERROR: UtilCreateProcessAndWait:702: /bin/mount failed with 2
[  +0.000193] init: (1) ERROR: UtilCreateProcessAndWait:722: /bin/mount failed with status 0x
[  +0.000001] ff00
[  +0.000002] init: (1) ERROR: ConfigMountFsTab:2484: Processing fstab with mount -a failed.
[  +0.009484] WARNING: /usr/share/zoneinfo/America/Sao_Paulo not found. Is the tzdata package installed?
[  +0.056619] init: (8) ERROR: CreateProcessEntryCommon:443: getpwuid(0) failed 2
[  +0.000004] init: (8) ERROR: CreateProcessEntryCommon:446: getpwuid(0) failed 2
[  +1.751865] cgroup: runc (594) created nested cgroup for controller "memory" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.
[  +0.000001] cgroup: "memory" requires setting use_hierarchy to 1 on the root

* 
* ==> etcd [4aa9ac44a818] <==
* {"level":"info","ts":"2024-02-10T11:55:48.948719Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-10T11:55:48.948741Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-10T11:55:48.949349Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-10T11:55:48.949415Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-10T11:55:49.827658Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 7"}
{"level":"info","ts":"2024-02-10T11:55:49.827959Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 7"}
{"level":"info","ts":"2024-02-10T11:55:49.827985Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-02-10T11:55:49.827998Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 8"}
{"level":"info","ts":"2024-02-10T11:55:49.828007Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-02-10T11:55:49.82802Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 8"}
{"level":"info","ts":"2024-02-10T11:55:49.828032Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-02-10T11:55:49.83349Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-10T11:55:49.833517Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-10T11:55:49.833538Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-10T11:55:49.834211Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-10T11:55:49.834276Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-10T11:55:49.837856Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-02-10T11:55:49.837868Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-02-10T12:05:49.941688Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31766}
{"level":"info","ts":"2024-02-10T12:05:49.958015Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31766,"took":"15.6067ms","hash":3543957752}
{"level":"info","ts":"2024-02-10T12:05:49.95809Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3543957752,"revision":31766,"compact-revision":31014}
{"level":"info","ts":"2024-02-10T12:06:18.744776Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":40004,"local-member-snapshot-index":30003,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-02-10T12:06:18.751254Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":40004}
{"level":"info","ts":"2024-02-10T12:06:18.751344Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":35004}
{"level":"info","ts":"2024-02-10T12:10:49.958154Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32066}
{"level":"info","ts":"2024-02-10T12:10:49.959201Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32066,"took":"838.9Âµs","hash":2625083876}
{"level":"info","ts":"2024-02-10T12:10:49.959255Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2625083876,"revision":32066,"compact-revision":31766}
{"level":"info","ts":"2024-02-10T12:15:49.964624Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32307}
{"level":"info","ts":"2024-02-10T12:15:49.965626Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32307,"took":"751.3Âµs","hash":854853472}
{"level":"info","ts":"2024-02-10T12:15:49.965679Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":854853472,"revision":32307,"compact-revision":32066}
{"level":"info","ts":"2024-02-10T12:20:49.970741Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32547}
{"level":"info","ts":"2024-02-10T12:20:49.971689Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32547,"took":"744Âµs","hash":1313629541}
{"level":"info","ts":"2024-02-10T12:20:49.97174Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1313629541,"revision":32547,"compact-revision":32307}
{"level":"info","ts":"2024-02-10T12:24:26.335073Z","caller":"traceutil/trace.go:171","msg":"trace[934314972] transaction","detail":"{read_only:false; response_revision:32960; number_of_response:1; }","duration":"108.0133ms","start":"2024-02-10T12:24:26.226129Z","end":"2024-02-10T12:24:26.334142Z","steps":["trace[934314972] 'process raft request'  (duration: 107.4434ms)"],"step_count":1}
{"level":"info","ts":"2024-02-10T12:25:49.977411Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32786}
{"level":"info","ts":"2024-02-10T12:25:49.979069Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32786,"took":"1.2324ms","hash":3865153223}
{"level":"info","ts":"2024-02-10T12:25:49.979212Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3865153223,"revision":32786,"compact-revision":32547}
{"level":"info","ts":"2024-02-10T12:30:49.984418Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33027}
{"level":"info","ts":"2024-02-10T12:30:49.985833Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33027,"took":"1.0144ms","hash":889673746}
{"level":"info","ts":"2024-02-10T12:30:49.985903Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":889673746,"revision":33027,"compact-revision":32786}
{"level":"info","ts":"2024-02-10T12:35:49.989594Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33266}
{"level":"info","ts":"2024-02-10T12:35:49.990382Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33266,"took":"575.4Âµs","hash":3355264303}
{"level":"info","ts":"2024-02-10T12:35:49.990428Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3355264303,"revision":33266,"compact-revision":33027}
{"level":"info","ts":"2024-02-10T12:40:50.004842Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33547}
{"level":"info","ts":"2024-02-10T12:40:50.006403Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33547,"took":"1.3314ms","hash":629357165}
{"level":"info","ts":"2024-02-10T12:40:50.006461Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":629357165,"revision":33547,"compact-revision":33266}
{"level":"info","ts":"2024-02-10T12:45:50.020155Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33787}
{"level":"info","ts":"2024-02-10T12:45:50.020928Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33787,"took":"535.2Âµs","hash":3982181621}
{"level":"info","ts":"2024-02-10T12:45:50.020974Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3982181621,"revision":33787,"compact-revision":33547}
{"level":"info","ts":"2024-02-10T12:48:06.324719Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-02-10T12:48:06.326934Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-02-10T12:48:06.327938Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-10T12:48:06.328571Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-10T12:48:06.329288Z","caller":"v3rpc/watch.go:473","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2024-02-10T12:48:06.43156Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-10T12:48:06.431654Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-02-10T12:48:06.432598Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-10T12:48:06.523767Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-10T12:48:06.524381Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-10T12:48:06.524411Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [5bbd7aab1fcb] <==
* {"level":"warn","ts":"2024-02-17T12:38:14.036955Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-02-17T12:38:14.037483Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-02-17T12:38:14.037524Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-02-17T12:38:14.037535Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-02-17T12:38:14.037541Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-17T12:38:14.037938Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-17T12:38:14.039479Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-02-17T12:38:14.039568Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-02-17T12:38:14.060672Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"20.9806ms"}
{"level":"info","ts":"2024-02-17T12:38:14.462008Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":40004,"snapshot-size":"9.3 kB"}
{"level":"info","ts":"2024-02-17T12:38:14.462068Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":3919872,"backend-size":"3.9 MB","backend-size-in-use-bytes":1224704,"backend-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-02-17T12:38:14.682846Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":42563}
{"level":"info","ts":"2024-02-17T12:38:14.683736Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-02-17T12:38:14.683775Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 8"}
{"level":"info","ts":"2024-02-17T12:38:14.683788Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 8, commit: 42563, applied: 40004, lastindex: 42563, lastterm: 8]"}
{"level":"info","ts":"2024-02-17T12:38:14.683906Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-17T12:38:14.683942Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-17T12:38:14.683961Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-02-17T12:38:14.686517Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-02-17T12:38:14.687421Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":33787}
{"level":"info","ts":"2024-02-17T12:38:14.753204Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":34137}
{"level":"info","ts":"2024-02-17T12:38:14.755368Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-02-17T12:38:14.757684Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-02-17T12:38:14.757897Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-17T12:38:14.757945Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-17T12:38:14.758062Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-02-17T12:38:14.758517Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-17T12:38:14.758808Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-17T12:38:14.758842Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-17T12:38:14.760027Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-17T12:38:14.760075Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-17T12:38:14.760708Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-17T12:38:14.76142Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-17T12:38:14.761477Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-17T12:38:15.384898Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 8"}
{"level":"info","ts":"2024-02-17T12:38:15.384958Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 8"}
{"level":"info","ts":"2024-02-17T12:38:15.384972Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-02-17T12:38:15.38498Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 9"}
{"level":"info","ts":"2024-02-17T12:38:15.384985Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 9"}
{"level":"info","ts":"2024-02-17T12:38:15.384991Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 9"}
{"level":"info","ts":"2024-02-17T12:38:15.384998Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 9"}
{"level":"info","ts":"2024-02-17T12:38:15.388657Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-17T12:38:15.388743Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-17T12:38:15.38878Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-17T12:38:15.389149Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-17T12:38:15.389187Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-17T12:38:15.390463Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-02-17T12:38:15.391546Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-02-17T12:48:15.417695Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34506}
{"level":"info","ts":"2024-02-17T12:48:15.432809Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":34506,"took":"14.7859ms","hash":2222004208}
{"level":"info","ts":"2024-02-17T12:48:15.43285Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2222004208,"revision":34506,"compact-revision":33787}
{"level":"info","ts":"2024-02-17T12:53:15.432589Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34821}
{"level":"info","ts":"2024-02-17T12:53:15.433292Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":34821,"took":"521.3Âµs","hash":4016345226}
{"level":"info","ts":"2024-02-17T12:53:15.433326Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4016345226,"revision":34821,"compact-revision":34506}
{"level":"info","ts":"2024-02-17T12:58:15.447678Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":35065}
{"level":"info","ts":"2024-02-17T12:58:15.448353Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":35065,"took":"493.4Âµs","hash":1785457409}
{"level":"info","ts":"2024-02-17T12:58:15.448388Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1785457409,"revision":35065,"compact-revision":34821}

* 
* ==> kernel <==
*  12:58:53 up 35 min,  0 users,  load average: 0.06, 0.20, 0.21
Linux minikube 5.4.72-microsoft-standard-WSL2 #1 SMP Wed Oct 28 23:40:43 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [373b0edb5828] <==
* E0217 12:48:56.694607       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E0217 12:49:06.694832       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:49:16.695797       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E0217 12:49:26.696006       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:49:36.696491       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E0217 12:49:46.697475       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:49:56.697679       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:50:06.698762       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:50:16.699797       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:50:26.700182       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0217 12:50:36.700741       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E0217 12:50:46.701929       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0217 12:50:56.702394       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:51:06.702741       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:51:16.703714       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:51:26.704300       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:51:36.705275       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:51:46.705509       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:51:56.706459       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0217 12:52:06.707641       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:52:16.708577       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:52:26.709541       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0217 12:52:36.709884       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:52:46.710507       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E0217 12:52:56.711869       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0217 12:53:06.712458       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:53:16.713485       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:53:26.714698       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:53:36.715486       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:53:46.716475       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:53:56.716955       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:54:06.717842       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:54:16.718683       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0217 12:54:26.719187       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:54:36.719439       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:54:46.720286       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E0217 12:54:56.721192       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0217 12:55:06.722488       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:55:16.723510       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:55:26.724002       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E0217 12:55:36.724560       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0217 12:55:46.725378       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E0217 12:55:56.726008       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0217 12:56:06.727173       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:56:16.727376       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:56:26.728231       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:56:36.728410       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E0217 12:56:46.729232       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0217 12:56:56.729597       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:57:06.730714       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:57:16.731508       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E0217 12:57:26.732495       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:57:36.733620       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0217 12:57:46.734097       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:57:56.734551       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0217 12:58:06.735583       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E0217 12:58:16.736496       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E0217 12:58:26.737444       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E0217 12:58:36.738304       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0217 12:58:46.739351       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]

* 
* ==> kube-apiserver [37482745d9af] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0210 12:48:07.424780       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0210 12:48:07.424804       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0210 12:48:07.424838       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0210 12:48:07.424868       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0210 12:48:07.424941       1 logging.go:59] [core] [Channel #16 SubChannel #17] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0210 12:48:07.424991       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0210 12:48:07.424992       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [6dd0f943bc1f] <==
* I0217 12:38:28.863546       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0217 12:38:28.864375       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0217 12:38:28.865846       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0217 12:38:28.868165       1 shared_informer.go:318] Caches are synced for endpoint
I0217 12:38:28.869364       1 shared_informer.go:318] Caches are synced for expand
I0217 12:38:28.871452       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0217 12:38:28.871565       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="47.9Âµs"
I0217 12:38:28.871566       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="56Âµs"
I0217 12:38:28.871645       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="70.5Âµs"
I0217 12:38:28.872716       1 shared_informer.go:318] Caches are synced for GC
I0217 12:38:28.879075       1 shared_informer.go:318] Caches are synced for persistent volume
I0217 12:38:28.879101       1 shared_informer.go:318] Caches are synced for attach detach
I0217 12:38:28.879200       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0217 12:38:28.891302       1 shared_informer.go:318] Caches are synced for ephemeral
I0217 12:38:28.897453       1 shared_informer.go:318] Caches are synced for node
I0217 12:38:28.897501       1 range_allocator.go:174] "Sending events to api server"
I0217 12:38:28.897516       1 range_allocator.go:178] "Starting range CIDR allocator"
I0217 12:38:28.897520       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0217 12:38:28.897523       1 shared_informer.go:318] Caches are synced for cidrallocator
I0217 12:38:28.899872       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0217 12:38:28.905821       1 shared_informer.go:318] Caches are synced for job
I0217 12:38:28.908160       1 shared_informer.go:318] Caches are synced for cronjob
I0217 12:38:28.911760       1 shared_informer.go:318] Caches are synced for ReplicationController
I0217 12:38:28.913043       1 shared_informer.go:318] Caches are synced for service account
I0217 12:38:28.916284       1 shared_informer.go:318] Caches are synced for TTL after finished
I0217 12:38:28.917410       1 shared_informer.go:318] Caches are synced for PVC protection
I0217 12:38:28.918618       1 shared_informer.go:318] Caches are synced for PV protection
I0217 12:38:28.920974       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0217 12:38:28.942789       1 shared_informer.go:318] Caches are synced for namespace
I0217 12:38:28.948273       1 shared_informer.go:318] Caches are synced for HPA
I0217 12:38:28.949482       1 shared_informer.go:318] Caches are synced for TTL
I0217 12:38:28.977460       1 shared_informer.go:318] Caches are synced for disruption
I0217 12:38:28.982030       1 shared_informer.go:318] Caches are synced for deployment
I0217 12:38:29.009972       1 shared_informer.go:318] Caches are synced for taint
I0217 12:38:29.010037       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0217 12:38:29.010108       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0217 12:38:29.010147       1 taint_manager.go:211] "Sending events to api server"
I0217 12:38:29.010180       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0217 12:38:29.010119       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0217 12:38:29.010221       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0217 12:38:29.083345       1 shared_informer.go:318] Caches are synced for stateful set
I0217 12:38:29.132983       1 shared_informer.go:318] Caches are synced for resource quota
I0217 12:38:29.152075       1 shared_informer.go:318] Caches are synced for resource quota
I0217 12:38:29.152324       1 shared_informer.go:318] Caches are synced for daemon sets
I0217 12:38:29.465287       1 shared_informer.go:318] Caches are synced for garbage collector
I0217 12:38:29.504637       1 shared_informer.go:318] Caches are synced for garbage collector
I0217 12:38:29.504668       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0217 12:38:51.477492       1 event.go:307] "Event occurred" object="default/app-desafio-deployment-647cf88bfd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-desafio-deployment-647cf88bfd-689rx"
I0217 12:38:51.482587       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="22.6129ms"
I0217 12:38:51.489549       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="6.9002ms"
I0217 12:38:51.489649       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="47.2Âµs"
I0217 12:38:51.494305       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="65Âµs"
I0217 12:38:52.331985       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="4.7972ms"
I0217 12:38:52.332047       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="33.9Âµs"
I0217 12:39:21.651203       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="65.5Âµs"
I0217 12:39:22.668839       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="131.3Âµs"
I0217 12:39:22.686309       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="69.5Âµs"
I0217 12:39:22.689327       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="43.1Âµs"
I0217 12:39:56.914070       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="6.7Âµs"
I0217 12:39:56.914608       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="4.1Âµs"

* 
* ==> kube-controller-manager [9a94d984dcb4] <==
* I0210 11:56:04.121409       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0210 11:56:04.121417       1 shared_informer.go:318] Caches are synced for cidrallocator
I0210 11:56:04.121449       1 shared_informer.go:318] Caches are synced for crt configmap
I0210 11:56:04.121564       1 shared_informer.go:318] Caches are synced for deployment
I0210 11:56:04.121810       1 shared_informer.go:318] Caches are synced for ephemeral
I0210 11:56:04.126898       1 shared_informer.go:318] Caches are synced for disruption
I0210 11:56:04.129037       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="8.1494ms"
I0210 11:56:04.130480       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0210 11:56:04.130531       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0210 11:56:04.130481       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0210 11:56:04.130506       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0210 11:56:04.133246       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0210 11:56:04.134438       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0210 11:56:04.141641       1 shared_informer.go:318] Caches are synced for daemon sets
I0210 11:56:04.218878       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0210 11:56:04.218924       1 shared_informer.go:318] Caches are synced for cronjob
I0210 11:56:04.219388       1 shared_informer.go:318] Caches are synced for taint
I0210 11:56:04.219628       1 shared_informer.go:318] Caches are synced for endpoint
I0210 11:56:04.219714       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0210 11:56:04.219780       1 taint_manager.go:211] "Sending events to api server"
I0210 11:56:04.219840       1 shared_informer.go:318] Caches are synced for job
I0210 11:56:04.219929       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0210 11:56:04.220638       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0210 11:56:04.221232       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0210 11:56:04.221417       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0210 11:56:04.243529       1 shared_informer.go:318] Caches are synced for PV protection
I0210 11:56:04.271823       1 shared_informer.go:318] Caches are synced for persistent volume
I0210 11:56:04.282872       1 shared_informer.go:318] Caches are synced for resource quota
I0210 11:56:04.318883       1 shared_informer.go:318] Caches are synced for resource quota
I0210 11:56:04.318953       1 shared_informer.go:318] Caches are synced for attach detach
I0210 11:56:04.632247       1 shared_informer.go:318] Caches are synced for garbage collector
I0210 11:56:04.632386       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0210 11:56:04.634515       1 shared_informer.go:318] Caches are synced for garbage collector
I0210 12:03:28.855967       1 event.go:307] "Event occurred" object="default/app-desafio-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set app-desafio-deployment-7585595468 to 1"
I0210 12:03:28.875435       1 event.go:307] "Event occurred" object="default/app-desafio-deployment-7585595468" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-desafio-deployment-7585595468-sq9lp"
I0210 12:03:28.882283       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="26.6183ms"
I0210 12:03:28.887722       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="5.3291ms"
I0210 12:03:28.887860       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="41.8Âµs"
I0210 12:03:28.898398       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="79.9Âµs"
I0210 12:03:28.932984       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="61.3Âµs"
I0210 12:03:33.658520       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="75.6Âµs"
I0210 12:03:46.759096       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="6.2169ms"
I0210 12:03:46.759614       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="436Âµs"
I0210 12:32:05.249393       1 event.go:307] "Event occurred" object="default/app-desafio-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set app-desafio-deployment-647cf88bfd to 1"
I0210 12:32:05.260566       1 event.go:307] "Event occurred" object="default/app-desafio-deployment-647cf88bfd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-desafio-deployment-647cf88bfd-5btcr"
I0210 12:32:05.266373       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="17.5023ms"
I0210 12:32:05.274372       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="7.8971ms"
I0210 12:32:05.274548       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="102.2Âµs"
I0210 12:32:05.328482       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="89.7Âµs"
I0210 12:32:06.183756       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="7.6552ms"
I0210 12:32:06.183898       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-647cf88bfd" duration="47.1Âµs"
I0210 12:32:06.190381       1 event.go:307] "Event occurred" object="default/app-desafio-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set app-desafio-deployment-7585595468 to 0 from 1"
I0210 12:32:06.198502       1 event.go:307] "Event occurred" object="default/app-desafio-deployment-7585595468" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: app-desafio-deployment-7585595468-sq9lp"
I0210 12:32:06.223362       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="33.6638ms"
I0210 12:32:06.230696       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="7.1199ms"
I0210 12:32:06.230967       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="93.5Âµs"
I0210 12:32:36.427286       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="72.6Âµs"
I0210 12:32:36.490802       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="45.7Âµs"
I0210 12:32:36.794821       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="78.4Âµs"
I0210 12:32:36.799685       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-desafio-deployment-7585595468" duration="79.1Âµs"

* 
* ==> kube-proxy [fb0bfb185814] <==
* I0210 11:55:57.052543       1 server_others.go:69] "Using iptables proxy"
I0210 11:55:57.170336       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0210 11:55:57.326456       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0210 11:55:57.329707       1 server_others.go:152] "Using iptables Proxier"
I0210 11:55:57.329795       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0210 11:55:57.329806       1 server_others.go:438] "Defaulting to no-op detect-local"
I0210 11:55:57.331167       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0210 11:55:57.333082       1 server.go:846] "Version info" version="v1.28.3"
I0210 11:55:57.333275       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0210 11:55:57.343242       1 config.go:188] "Starting service config controller"
I0210 11:55:57.343251       1 config.go:315] "Starting node config controller"
I0210 11:55:57.343252       1 config.go:97] "Starting endpoint slice config controller"
I0210 11:55:57.343952       1 shared_informer.go:311] Waiting for caches to sync for service config
I0210 11:55:57.343953       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0210 11:55:57.343955       1 shared_informer.go:311] Waiting for caches to sync for node config
I0210 11:55:57.446179       1 shared_informer.go:318] Caches are synced for node config
I0210 11:55:57.446233       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0210 11:55:57.446267       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [fbedc6f5ed44] <==
* I0217 12:38:18.275409       1 server_others.go:69] "Using iptables proxy"
I0217 12:38:18.299640       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0217 12:38:18.376685       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0217 12:38:18.378799       1 server_others.go:152] "Using iptables Proxier"
I0217 12:38:18.378866       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0217 12:38:18.378879       1 server_others.go:438] "Defaulting to no-op detect-local"
I0217 12:38:18.380662       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0217 12:38:18.380975       1 server.go:846] "Version info" version="v1.28.3"
I0217 12:38:18.381004       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0217 12:38:18.382735       1 config.go:188] "Starting service config controller"
I0217 12:38:18.382792       1 config.go:97] "Starting endpoint slice config controller"
I0217 12:38:18.382855       1 config.go:315] "Starting node config controller"
I0217 12:38:18.384753       1 shared_informer.go:311] Waiting for caches to sync for node config
I0217 12:38:18.384772       1 shared_informer.go:311] Waiting for caches to sync for service config
I0217 12:38:18.384783       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0217 12:38:18.485138       1 shared_informer.go:318] Caches are synced for service config
I0217 12:38:18.485147       1 shared_informer.go:318] Caches are synced for node config
I0217 12:38:18.485162       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [20b3f7a21777] <==
* I0217 12:38:15.196257       1 serving.go:348] Generated self-signed cert in-memory
I0217 12:38:16.673761       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0217 12:38:16.673808       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0217 12:38:16.678331       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0217 12:38:16.678370       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0217 12:38:16.678603       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0217 12:38:16.678976       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0217 12:38:16.679230       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0217 12:38:16.679271       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0217 12:38:16.679920       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0217 12:38:16.680294       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0217 12:38:16.780843       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0217 12:38:16.780851       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0217 12:38:16.780868       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController

* 
* ==> kube-scheduler [af70fe2effca] <==
* I0210 11:55:48.033421       1 serving.go:348] Generated self-signed cert in-memory
W0210 11:55:51.425474       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0210 11:55:51.425604       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0210 11:55:51.425632       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0210 11:55:51.425643       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0210 11:55:51.545941       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0210 11:55:51.545993       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0210 11:55:51.550904       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0210 11:55:51.551688       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0210 11:55:51.552130       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0210 11:55:51.552357       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0210 11:55:51.652018       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0210 12:48:06.133733       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0210 12:48:06.133629       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0210 12:48:06.136252       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Feb 17 12:40:27 minikube kubelet[1642]: I0217 12:40:27.128785    1642 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/790c25d7-4b3f-45dc-a339-069efa372734-kube-api-access-4bz2f" (OuterVolumeSpecName: "kube-api-access-4bz2f") pod "790c25d7-4b3f-45dc-a339-069efa372734" (UID: "790c25d7-4b3f-45dc-a339-069efa372734"). InnerVolumeSpecName "kube-api-access-4bz2f". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 17 12:40:27 minikube kubelet[1642]: I0217 12:40:27.227957    1642 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-4bz2f\" (UniqueName: \"kubernetes.io/projected/790c25d7-4b3f-45dc-a339-069efa372734-kube-api-access-4bz2f\") on node \"minikube\" DevicePath \"\""
Feb 17 12:40:27 minikube kubelet[1642]: I0217 12:40:27.227994    1642 reconciler_common.go:300] "Volume detached for volume \"app-desafio-pv\" (UniqueName: \"kubernetes.io/empty-dir/790c25d7-4b3f-45dc-a339-069efa372734-app-desafio-pv\") on node \"minikube\" DevicePath \"\""
Feb 17 12:40:27 minikube kubelet[1642]: I0217 12:40:27.282847    1642 scope.go:117] "RemoveContainer" containerID="b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694"
Feb 17 12:40:27 minikube kubelet[1642]: I0217 12:40:27.296916    1642 scope.go:117] "RemoveContainer" containerID="b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694"
Feb 17 12:40:27 minikube kubelet[1642]: E0217 12:40:27.297769    1642 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694" containerID="b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694"
Feb 17 12:40:27 minikube kubelet[1642]: I0217 12:40:27.297815    1642 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694"} err="failed to get container status \"b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694\": rpc error: code = Unknown desc = Error response from daemon: No such container: b631d39125a8b4b6a65c3ded1b7c24217f669d81923731f5510087db012a0694"
Feb 17 12:40:28 minikube kubelet[1642]: I0217 12:40:28.861076    1642 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="790c25d7-4b3f-45dc-a339-069efa372734" path="/var/lib/kubelet/pods/790c25d7-4b3f-45dc-a339-069efa372734/volumes"
Feb 17 12:43:12 minikube kubelet[1642]: W0217 12:43:12.871195    1642 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 17 12:43:53 minikube kubelet[1642]: I0217 12:43:53.617906    1642 topology_manager.go:215] "Topology Admit Handler" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791" podNamespace="default" podName="kubernetes-2-desafio-pod"
Feb 17 12:43:53 minikube kubelet[1642]: E0217 12:43:53.618072    1642 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="ab825152-ca7e-4045-b84f-17d9815f338b" containerName="app-desafio-deployment"
Feb 17 12:43:53 minikube kubelet[1642]: E0217 12:43:53.618094    1642 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="790c25d7-4b3f-45dc-a339-069efa372734" containerName="app-desafio-deployment"
Feb 17 12:43:53 minikube kubelet[1642]: I0217 12:43:53.618227    1642 memory_manager.go:346] "RemoveStaleState removing state" podUID="ab825152-ca7e-4045-b84f-17d9815f338b" containerName="app-desafio-deployment"
Feb 17 12:43:53 minikube kubelet[1642]: I0217 12:43:53.618242    1642 memory_manager.go:346] "RemoveStaleState removing state" podUID="790c25d7-4b3f-45dc-a339-069efa372734" containerName="app-desafio-deployment"
Feb 17 12:43:53 minikube kubelet[1642]: I0217 12:43:53.618248    1642 memory_manager.go:346] "RemoveStaleState removing state" podUID="ab825152-ca7e-4045-b84f-17d9815f338b" containerName="app-desafio-deployment"
Feb 17 12:43:53 minikube kubelet[1642]: I0217 12:43:53.790955    1642 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-59rjd\" (UniqueName: \"kubernetes.io/projected/2f4ae103-4b13-4cb7-a14e-dd00883b7791-kube-api-access-59rjd\") pod \"kubernetes-2-desafio-pod\" (UID: \"2f4ae103-4b13-4cb7-a14e-dd00883b7791\") " pod="default/kubernetes-2-desafio-pod"
Feb 17 12:43:56 minikube kubelet[1642]: E0217 12:43:56.358601    1642 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown" image="joohnsro/kubernetes-2-desafio:latest"
Feb 17 12:43:56 minikube kubelet[1642]: E0217 12:43:56.358775    1642 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown" image="joohnsro/kubernetes-2-desafio:latest"
Feb 17 12:43:56 minikube kubelet[1642]: E0217 12:43:56.360352    1642 kuberuntime_manager.go:1256] container &Container{Name:kubernetes-2-desafio-pod,Image:joohnsro/kubernetes-2-desafio,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59rjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod kubernetes-2-desafio-pod_default(2f4ae103-4b13-4cb7-a14e-dd00883b7791): ErrImagePull: Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown
Feb 17 12:43:56 minikube kubelet[1642]: E0217 12:43:56.360454    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ErrImagePull: \"Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:43:57 minikube kubelet[1642]: E0217 12:43:57.367901    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:44:11 minikube kubelet[1642]: E0217 12:44:11.742804    1642 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown" image="joohnsro/kubernetes-2-desafio:latest"
Feb 17 12:44:11 minikube kubelet[1642]: E0217 12:44:11.742845    1642 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown" image="joohnsro/kubernetes-2-desafio:latest"
Feb 17 12:44:11 minikube kubelet[1642]: E0217 12:44:11.742923    1642 kuberuntime_manager.go:1256] container &Container{Name:kubernetes-2-desafio-pod,Image:joohnsro/kubernetes-2-desafio,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59rjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod kubernetes-2-desafio-pod_default(2f4ae103-4b13-4cb7-a14e-dd00883b7791): ErrImagePull: Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown
Feb 17 12:44:11 minikube kubelet[1642]: E0217 12:44:11.742947    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ErrImagePull: \"Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:44:24 minikube kubelet[1642]: E0217 12:44:24.859436    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:44:41 minikube kubelet[1642]: E0217 12:44:41.725012    1642 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown" image="joohnsro/kubernetes-2-desafio:latest"
Feb 17 12:44:41 minikube kubelet[1642]: E0217 12:44:41.725054    1642 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown" image="joohnsro/kubernetes-2-desafio:latest"
Feb 17 12:44:41 minikube kubelet[1642]: E0217 12:44:41.725134    1642 kuberuntime_manager.go:1256] container &Container{Name:kubernetes-2-desafio-pod,Image:joohnsro/kubernetes-2-desafio,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59rjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod kubernetes-2-desafio-pod_default(2f4ae103-4b13-4cb7-a14e-dd00883b7791): ErrImagePull: Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown
Feb 17 12:44:41 minikube kubelet[1642]: E0217 12:44:41.725163    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ErrImagePull: \"Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:44:53 minikube kubelet[1642]: E0217 12:44:53.860067    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:45:04 minikube kubelet[1642]: E0217 12:45:04.860171    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:45:18 minikube kubelet[1642]: E0217 12:45:18.859228    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:45:32 minikube kubelet[1642]: E0217 12:45:32.921350    1642 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown" image="joohnsro/kubernetes-2-desafio:latest"
Feb 17 12:45:32 minikube kubelet[1642]: E0217 12:45:32.921397    1642 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown" image="joohnsro/kubernetes-2-desafio:latest"
Feb 17 12:45:32 minikube kubelet[1642]: E0217 12:45:32.921505    1642 kuberuntime_manager.go:1256] container &Container{Name:kubernetes-2-desafio-pod,Image:joohnsro/kubernetes-2-desafio,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59rjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod kubernetes-2-desafio-pod_default(2f4ae103-4b13-4cb7-a14e-dd00883b7791): ErrImagePull: Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown
Feb 17 12:45:32 minikube kubelet[1642]: E0217 12:45:32.921532    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ErrImagePull: \"Error response from daemon: manifest for joohnsro/kubernetes-2-desafio:latest not found: manifest unknown: manifest unknown\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:45:45 minikube kubelet[1642]: E0217 12:45:45.860646    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:45:58 minikube kubelet[1642]: E0217 12:45:58.860041    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:46:09 minikube kubelet[1642]: E0217 12:46:09.858984    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:46:24 minikube kubelet[1642]: E0217 12:46:24.860747    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:46:39 minikube kubelet[1642]: E0217 12:46:39.859488    1642 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-2-desafio-pod\" with ImagePullBackOff: \"Back-off pulling image \\\"joohnsro/kubernetes-2-desafio\\\"\"" pod="default/kubernetes-2-desafio-pod" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791"
Feb 17 12:46:55 minikube kubelet[1642]: I0217 12:46:55.166726    1642 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/kubernetes-2-desafio-pod" podStartSLOduration=2.2043407 podCreationTimestamp="2024-02-17 12:43:53 +0000 UTC" firstStartedPulling="2024-02-17 12:43:54.0795309 +0000 UTC m=+341.469442901" lastFinishedPulling="2024-02-17 12:46:54.0424012 +0000 UTC m=+521.431795101" observedRunningTime="2024-02-17 12:46:55.1666005 +0000 UTC m=+522.555994401" watchObservedRunningTime="2024-02-17 12:46:55.1666929 +0000 UTC m=+522.556086801"
Feb 17 12:47:08 minikube kubelet[1642]: I0217 12:47:08.033664    1642 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-59rjd\" (UniqueName: \"kubernetes.io/projected/2f4ae103-4b13-4cb7-a14e-dd00883b7791-kube-api-access-59rjd\") pod \"2f4ae103-4b13-4cb7-a14e-dd00883b7791\" (UID: \"2f4ae103-4b13-4cb7-a14e-dd00883b7791\") "
Feb 17 12:47:08 minikube kubelet[1642]: I0217 12:47:08.035404    1642 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/2f4ae103-4b13-4cb7-a14e-dd00883b7791-kube-api-access-59rjd" (OuterVolumeSpecName: "kube-api-access-59rjd") pod "2f4ae103-4b13-4cb7-a14e-dd00883b7791" (UID: "2f4ae103-4b13-4cb7-a14e-dd00883b7791"). InnerVolumeSpecName "kube-api-access-59rjd". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 17 12:47:08 minikube kubelet[1642]: I0217 12:47:08.134860    1642 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-59rjd\" (UniqueName: \"kubernetes.io/projected/2f4ae103-4b13-4cb7-a14e-dd00883b7791-kube-api-access-59rjd\") on node \"minikube\" DevicePath \"\""
Feb 17 12:47:08 minikube kubelet[1642]: I0217 12:47:08.263689    1642 scope.go:117] "RemoveContainer" containerID="d34c80773b8105ca5dcf5a693aaf6f18e1eb748a3711bfff3660c328ce26ca2d"
Feb 17 12:47:08 minikube kubelet[1642]: I0217 12:47:08.287880    1642 scope.go:117] "RemoveContainer" containerID="d34c80773b8105ca5dcf5a693aaf6f18e1eb748a3711bfff3660c328ce26ca2d"
Feb 17 12:47:08 minikube kubelet[1642]: E0217 12:47:08.288954    1642 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: d34c80773b8105ca5dcf5a693aaf6f18e1eb748a3711bfff3660c328ce26ca2d" containerID="d34c80773b8105ca5dcf5a693aaf6f18e1eb748a3711bfff3660c328ce26ca2d"
Feb 17 12:47:08 minikube kubelet[1642]: I0217 12:47:08.289002    1642 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"d34c80773b8105ca5dcf5a693aaf6f18e1eb748a3711bfff3660c328ce26ca2d"} err="failed to get container status \"d34c80773b8105ca5dcf5a693aaf6f18e1eb748a3711bfff3660c328ce26ca2d\": rpc error: code = Unknown desc = Error response from daemon: No such container: d34c80773b8105ca5dcf5a693aaf6f18e1eb748a3711bfff3660c328ce26ca2d"
Feb 17 12:47:08 minikube kubelet[1642]: I0217 12:47:08.861320    1642 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791" path="/var/lib/kubelet/pods/2f4ae103-4b13-4cb7-a14e-dd00883b7791/volumes"
Feb 17 12:47:16 minikube kubelet[1642]: I0217 12:47:16.096808    1642 topology_manager.go:215] "Topology Admit Handler" podUID="104a54ee-1b4c-4b40-bf41-a49a0d67a498" podNamespace="default" podName="kubernetes-2-desafio-pod"
Feb 17 12:47:16 minikube kubelet[1642]: E0217 12:47:16.096922    1642 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="ab825152-ca7e-4045-b84f-17d9815f338b" containerName="app-desafio-deployment"
Feb 17 12:47:16 minikube kubelet[1642]: E0217 12:47:16.096938    1642 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791" containerName="kubernetes-2-desafio-pod"
Feb 17 12:47:16 minikube kubelet[1642]: I0217 12:47:16.096964    1642 memory_manager.go:346] "RemoveStaleState removing state" podUID="2f4ae103-4b13-4cb7-a14e-dd00883b7791" containerName="kubernetes-2-desafio-pod"
Feb 17 12:47:16 minikube kubelet[1642]: I0217 12:47:16.185298    1642 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vsxz8\" (UniqueName: \"kubernetes.io/projected/104a54ee-1b4c-4b40-bf41-a49a0d67a498-kube-api-access-vsxz8\") pod \"kubernetes-2-desafio-pod\" (UID: \"104a54ee-1b4c-4b40-bf41-a49a0d67a498\") " pod="default/kubernetes-2-desafio-pod"
Feb 17 12:47:17 minikube kubelet[1642]: I0217 12:47:17.344735    1642 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/kubernetes-2-desafio-pod" podStartSLOduration=1.3446792 podCreationTimestamp="2024-02-17 12:47:16 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-02-17 12:47:17.3445902 +0000 UTC m=+544.734026101" watchObservedRunningTime="2024-02-17 12:47:17.3446792 +0000 UTC m=+544.734115001"
Feb 17 12:48:12 minikube kubelet[1642]: W0217 12:48:12.871600    1642 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 17 12:53:12 minikube kubelet[1642]: W0217 12:53:12.871700    1642 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 17 12:58:12 minikube kubelet[1642]: W0217 12:58:12.870946    1642 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [7e2645562d63] <==
* I0217 12:38:32.941811       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0217 12:38:32.950328       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0217 12:38:32.950659       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0217 12:38:50.341749       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0217 12:38:50.341813       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"54b48fec-606d-4f65-9723-746663d819cc", APIVersion:"v1", ResourceVersion:"34247", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f3f2ec37-7b48-4fa6-b6b6-312bd473f73c became leader
I0217 12:38:50.341842       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f3f2ec37-7b48-4fa6-b6b6-312bd473f73c!
I0217 12:38:50.443102       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f3f2ec37-7b48-4fa6-b6b6-312bd473f73c!

* 
* ==> storage-provisioner [db8dc47c4876] <==
* I0217 12:38:18.059287       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0217 12:38:18.072996       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

